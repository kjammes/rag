from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# ğŸ”§ Disable cloud LLM usage
Settings.llm = None

# ğŸ“„ Load documents from the "data" folder
documents = SimpleDirectoryReader("data").load_data()

# ğŸ¤– Use a local HuggingFace embedding model
embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ğŸ§  Create index
index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)

# ğŸ§  Query engine based on vector similarity
query_engine = index.as_query_engine()

print("âœ… PDF loaded and indexed. Ask me anything! (type 'exit' to quit)\n")

# ğŸ” Interactive loop
while True:
    query = input("â“ Your question: ")
    if query.lower() in ("exit", "quit"):
        print("ğŸ‘‹ Exiting. Have a great day!")
        break

    response = query_engine.query(query)
    print(f"ğŸ’¡ Answer: {response}\n")

